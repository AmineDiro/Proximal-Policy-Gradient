{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aaea0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b52e9dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "hidden_size=32\n",
    "lr=1e-2\n",
    "epochs=2\n",
    "batch_size=5000\n",
    "render=True\n",
    "use_cuda=False\n",
    "\n",
    "output_path_model='./models'\n",
    "save_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4430f5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3eb17d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "env = gym.make('CartPole-v0')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "n_acts = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02879396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init action network\n",
    "actor = Actor(obs_dim,hidden_size,n_acts).to(device)\n",
    "\n",
    "# make optimizer\n",
    "optimizer = Adam(actor.logits_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0da014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make loss function whose gradient, for the right data, is policy gradient\n",
    "def compute_loss(obs, act, weights):\n",
    "    logp = actor._log_prob_from_distribution(obs,act)\n",
    "    return -(logp * weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b668569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One epoch training \n",
    "def train_one_epoch():\n",
    "    # make some empty lists for logging.\n",
    "    batch_obs = []          # for observations\n",
    "    batch_acts = []         # for actions\n",
    "    batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "    batch_rets = []         # for measuring episode returns\n",
    "    batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "    # reset episode-specific variables\n",
    "    obs = env.reset()       # first obs comes from starting distribution\n",
    "    done = False            # signal from environment that episode is over\n",
    "    ep_rews = []            # list for rewards accrued throughout ep\n",
    "\n",
    "    # render first episode of each epoch\n",
    "    finished_rendering_this_epoch = False\n",
    "\n",
    "    # collect experience by acting in the environment with current policy\n",
    "    while True:\n",
    "        # rendering\n",
    "        if (not finished_rendering_this_epoch) and render:\n",
    "            env.render()\n",
    "\n",
    "        # save obs\n",
    "        batch_obs.append(obs.copy())\n",
    "\n",
    "        # act in the environment\n",
    "        obs_cuda= torch.as_tensor(obs, dtype=torch.float32,device=device)\n",
    "        act = actor.step(obs_cuda)\n",
    "        # Step then get state, reward and if done\n",
    "        obs, rew, done, _ = env.step(act)\n",
    "\n",
    "        # save action, reward\n",
    "        batch_acts.append(act.item())\n",
    "        ep_rews.append(rew)\n",
    "\n",
    "        if done:\n",
    "            # if episode is over, record info about episode\n",
    "            ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "            batch_rets.append(ep_ret)\n",
    "            batch_lens.append(ep_len)\n",
    "\n",
    "            # the weight for each logprob(a|s) == R(episode)\n",
    "            batch_weights += [ep_ret] * ep_len\n",
    "\n",
    "            # reset episode-specific variables\n",
    "            obs, done, ep_rews = env.reset(), False, []\n",
    "\n",
    "            # won't render again this epoch\n",
    "            finished_rendering_this_epoch = True\n",
    "\n",
    "            # end experience loop if we have enough of it\n",
    "            if len(batch_obs) > batch_size:\n",
    "                break\n",
    "        env.close()\n",
    "\n",
    "    # take a single policy gradient update step\n",
    "    optimizer.zero_grad()\n",
    "    batch_obs = torch.as_tensor(batch_obs, dtype=torch.float32,device =device)\n",
    "    batch_acts = torch.as_tensor(batch_acts, dtype=torch.float32,device=device)\n",
    "    weights = torch.as_tensor(batch_weights, dtype=torch.float32,device =device)\n",
    "\n",
    "    batch_loss = compute_loss(obs=batch_obs,\n",
    "                              act=batch_acts,\n",
    "                              weights=weights\n",
    "                              )\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # \n",
    "    # Save model ever N epochs\n",
    "    if (epoch % save_epochs == 0):\n",
    "        torch.save(\n",
    "                {\n",
    "                    \"actor\": actor.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                },\n",
    "                os.path.join(output_path_model, \"simple_pg_model.pth\"))\n",
    "            \n",
    "    return batch_loss, batch_rets, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb518370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 \t loss: 17.328 \t return: 19.952 \t ep_len: 19.952\n",
      "epoch:   1 \t loss: 19.206 \t return: 21.765 \t ep_len: 21.765\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "batch_avg_len=[]\n",
    "batch_avg_return=[]\n",
    "\n",
    "for i in range(epochs):\n",
    "    batch_loss, batch_rets, batch_lens = train_one_epoch()\n",
    "    batch_avg_len.append(np.mean(batch_lens))\n",
    "    batch_avg_return.append(np.mean(batch_avg_return))\n",
    "    print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "            (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d46e7af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppo",
   "language": "python",
   "name": "ppo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
